---
layout: post
title: "Notes for CS61b2"
categories: [Basics]
---

From lecture 20, CS61b begin to introduce real problems with data structures including *set, map, priority queue ADTs*.

## Lecture 20: Disjoint Sets

### Dynamic connectivity
Given a set of pairwise connectedness declarations, determine if two items are connected transitively, return yes or no.

#### Disjoint Sets ADT
```java
public interface DisjointSets {
	/** Connects two items p and q. */
	void connect(int p, int q);
	/** Check to see if two items are connected. */
	boolean isConnected(int p, int q);
}
```

Two ways of implementing connect:
1. Naive way: record single connections somehow;
2. Model connectedness in terms of sets: divide items into several connected components which are maximal sets of items that are mutually connected.

#### Quick Find
to support tracking of sets.

1. Map\<Integer, SetGuy\>: map the integer to the set it belongs to;
2. List\<HashSet\>: move things around between sets, but require iterating to find something;
3. **Using an array**: id[i] returns the set ID to which the ith item belongs; isConnected is fast while connect is slow (\Theta(n)).

#### Quick Union
to improve the Connect operation.

#### Weighted Quick Union
modify quick union to avoid tall trees.
- Track tree size;
- Rule: link root of *smaller* tree to *larger* tree.

Changes:
- Use parent[] as before, but add size[];
- isConnected(p, q) requires no changes;
- connect(p, q) has two changes:
	- link root of smaller tree to larger tree;
	- update size[].

```java
public void connect(int p, int q) {
	int i = find(p);
	int j = find(q);
	if (size[i] < size[j]){
		parent[i] = j;
		size[j] += size[i];
	} else {
		parent[j] = i;
		size[i] += size[j];
	}
}
```

Now we have \Omega(log N) for both connect and isConnceted, but we can do even better.

#### Path Compression
When calling isConnected, tie all nodes **seen** to the root, resulting in a union/connected operations close enough to amortized constant time.

This is also the standard way disjoint sets are implemented today - **quick union and path compression**.

Resulting find code:
```java
private int find(p) {
	if (p == parent[p]) {
		return p;
	} else {
		parent[p] = find(parent[p]);
		return parent[p];
	}
}
```

## Lecture 21: Binary Search Trees (BST)

Rooted (binary) tree:
- Every node X except the root has exactly one parent, which is the first node on the path from X to the root;
- The root is depicted at the top of the tree;
- A node with no child is called a *leaf*.
- (every node has either 0, 1, or 2 children (subtrees).

A BST is a rooted binary tree with the BST property.

**BST property**. For every node X in the tree:
- Every key in the **left** subtree is **less** than X’s key;
- Every key in the **right** subtree is **greater** than X’s key.

> Which also means **No duplicate keys allowed!**

### BST Operations
1. Find a search key in a BST
```java
static BST find(BST T, Key sk) {
	if (T == null)
		return null;
	if sk.keyEquals(T.label())
		return T;
	else if (sk < T.label())      //<T, go to find in left subtree
		return find(T.left, sk);
	else
		return find(T.right, sk); //>T, go to find in right subtree
}
```

2. Insert a new key into a BST
```java
static BST insert(BST T, Key ik) {
	if (T == null)			       //null, build a new BST;
		return new BST(ik);
	if (ik < T.label())		       //<T, insert into left subtree;
		T.left = insert(T.left, ik);
	else if (ik > T.label())       //>T, insert into right subtree;
		T.right = insert(T.right, ik);
	return T;					   //=T, exist, no change.
}
```

3. Delete from a BST: 3 Cases
	1. Deletion key has no children:
	2. Just sever the parent’s link.
	2. Deletion key has one child:
	4. Sever the parent’s link;
	5. Move the parent’s pointer to its child to keep BST property.
	3. Deletion key has two children:
	7. need to find a new root node to replace this root node of the subtree and keep the BST property.
	8. Two node satisfies the goal: the largest one of the left subtree, the smallest one of the right subtree.
	9. Delete one of the two nodes (using case 1/2) and stick a new copy in the root position. (known as **Hibbard deletion**)
```java
static BST delete(BST T, Key dk) {
	BST target = find(dk);
	if (T == null || target == null)
		return null;
	
	
```

### BST Performance
BST: efficient data structures for supporting insertion and search.
- Operations on “Bushy” BSTs are logarithmic time.
- Insertion of random data yields a bushy BST: on random data, order of growth for get/put operations is logarithmic.

Performance issues:
- “Spindly” trees have linear performance.
- Hibbard deletion results in order of growth that is sqrt(N).

## Lecture 22: Balanced Search Trees
> Last version of BSTs has potential performance issue when they get “spindly” (too tall).
> Worst case: Items are inserted in order.
> Last solution: insert in random order, resulting in \Theta(log N) height with extremely high probability.
> Potential issue: might not have all data up front.

### Tree Rotation
rotate a BST’s node to left or right.
- RotateLeft(G): G moves left, promote its right child (P for example). Changes we need to make:
	- P moves left too, take up the original position of G;
	- P is the new parent of G while G is the new left child of P;
	- the original left child of P becomes the new right child of G.
- RotateRight(P): G moves right, promote its left child (G for example). Changes we need to make:
	- G moves right, becoming the new root and parent of P;
	- the original right child of G becomes the new left child of P.

> rotateLeft and rotateRight is a pair of reversed operations.
> Rotation allows balancing of any BSTs; one way to achieve balance is **Rotating after each insertion and deletion to maintain balance.**
> The point is to know which rotations.

### B-trees / 2-3 trees / 2-3-4 trees
Types of search trees:
- Binary search trees: require rotations to maintain balance. Strategies for rotation include ALV, weight-balancing, **red-black**.
- Treaps.
- Splay trees.
- **2-3 / 2-3-4 trees / B-trees**: no rotations required.

# **Something needed to be added**

B-trees are popular in two specific areas:
1. Small *M* (M=3 or 4): used as a conceptually simple balanced search tree (as we used now);
2. Very large *M* (1000+): used in practice for databases and filesystems.
> Small M tree isn’t very useful, people usually only use “B-tree” to refer to the very large M case.

### Red-Black Trees
2-3 / 2-3-4 trees are hard to *implement*, and suffer from *performance issues* including:
- Maintain different node types (with different numbers of child nodes).
- Interconversion of nodes between 2-nodes and 3-nodes.
- Walk up the tree to split nodes.

Solution: Build a BST that is isometric to a 2-3 tree, using rotations to ensure the isometry.

**Dealing with 3-Nodes in a 2-3 Tree like BST**: create “glue” links with the smaller item *off to the left*. Marking glue link as “**red**”.

#### Left-Leaning Red Black Binary Search Tree (LLRB BST)

Properties that any 2-3 tree like BST obeys:
- No node has two red links (otherwise it’d be like a 4 node).
- Every path from root to a leaf has same number of black links.
- Red links lean left (just in this class).
- Also called “LLRB BSTs”.

A fact: For any 2-3 tree, there exists a corresponding red-black tree that has depth **no more than 2 times** the depth of the 2-3 tree.

#### Maintaining Isometry Through Rotations
If putting red links of a LLRB horizontally, we’ll find there exists an isometry between LLRB and 2-3 tree. And implementation of an LLRB is also based on maintaining this isometry.

When performing LLRB operations, pretending you’re a 2-3 tree. So using **Red Link** (not black link) when inserting, because in 2-3 trees we always start by increasing node size, which means it’s a red link. 

**Case 1: Right-Insert**
For example, we have leaf E, and insert S with red link. But S will become the right child of E.
*Using rotateLeft(E) to fix the issue and keep isometry*.

**Case 2: two red children**
Suppose we add F to an LLRB tree containing E and A, which are connected with a red link. F will be *right-inserted* and become A’s second red (also right) child.
*change the color of links between E&A, E&F into black, and E&its parent into red*.

Why? Because in a 2-3 tree, A&E are in the same node, and F will also be inserted into this node. Now splitting happens. A&F become two individual nodes, and E enter the node of its parent (means a red link between E and its parent node).

**Case 3: two reds-in-a-row**
Suppose we have A, E, F nodes, red linked left to right, bottom to top. This is also a case corresponding a 3-node which means splitting should happen.
*rotateRight(F) and change link colors like case 2*.

**Case 4: left red followed by a right insert**
Case 4 is a situation in which Case 1&3 happened simultaneously.
*rotateLeft(E) to Case 3, rotateRight(F) to Case 2, change colors like case 2*.

## Lecture 23: Hashing
Techniques for storing data, as well as their computing complexity of contain and insert:
- Ordered Linked List: \Theta(N), \Theta(N);
- Bushy BST: \Theta(log N), \Theta(log N);
- Unordered Array: \Theta(N), \Theta(N).

More fast: Use data as index (suppose store int only), store true/false as data (for contain operation) (\Theta(1), \Theta(1)).
Downside: wasteful of memory, to support all positive ints, need 2B booleans.
> We name this structure as DataIndexedIntegerSet (diis).

### Collision Handling
how to resolve ambiguity.

Suppose N items have the same hashcode h. We store a list of N items at position h instead of storing true. We call it **External Chaining**. Such a position storing a list is also called **bucket**.
> We use modulus of hashcode to reduce bucket count.

Suppose N items are distributed across M buckets, N/M=L is the average time, known as **load factor**. (Average runtime is \Theta(L))

To keep high performances of the structure, we need to increase *M*, the number of bucket, by resizing whenever **L=N/M** exceeds some number.

**The structure we are introducing is called *hash table* actually!**
- Every item is mapped to a bucket using a *hash function*;
- Computing hash function consists of two steps:
	1. Computing a hashCode (integer);
	2. computing index = hashCode mod M (ensure index is positive).
- Increase M if L=N/M is too large.

*External Chaining*, as well as *Open Addressing* can be used to resolve ambiguity.

> Performance depends on the number of items in each ‘bucket’, so the average runtime is just \Theta(L) so far. Next balanced buckets will be considered.

### Hash Functions

Default *hashCode()* will return *this*, the address of object.

Java provides a hash table based implementation of sets and maps.

Never store **mutable** objects in a HashSet|HashMap, since its hashcode can be modified, so should the bucket it belongs to.

Must overriding hashCode() when overridding equals().

### Extra: Open Addressing

If target bucket is occupied, instead of creating a list, we use a different bucket to store this item:
- Linear probing: use next address, if still not available, scan one by one;
- Quadratic probing: scan next as order of 1, 4, 9, 16, …
- More others.

## Lecture 24: Heaps and Priority Queue

### Priority Queues

PQs are useful if you want to keep track of the “smallest”, “largest”, “best”, etc.
```java
/** Interface of (Min) Priority Queue: allowing tracking and romoval of the smallest item in a priority queue. */
public interface MinPQ<Item> {
	/** Adds an item to the PQ. */
	public void add(Item x);
	/** Returns the smallest item of the PQ. */
	public Item getSmallest();
	/** Removes the smallest item of the PQ. */
	public Item removeSmallest();
	/** Returns the size of the PQ. */
	public int size();
}
```

What data structures we use to implement a MinPQ?
- Ordered Array (time-consuming if keep in order at any time)
- Bushy BST (work indeed, but handling duplicate priorities is awkward.)
- HashTable (no good. Items go into random places.)

### Heaps
Binary min-heap: binary tree that is complete and obeys **min-heap property**:
- Min-heap: every node is less than or equal to both of its children.
- Complete: missing items/nodes only at the bottom level if any.

Heap operations:
- getSmallest(): return the item in the root node.
- add(x): place x in the last position, then promote as high as possible.
- removeSmallest(): replace the item in the root node with value in the rightmost node, then demote repeatedly, always taking the ‘better’ successor.

### Tree Representations

1. Create mapping from node to children:
	1. Fixed-Width Nodes (BSTMap for example);
	2. Variable-Width Nodes;
	3. Sibling Tree (left-child, right-sibling).
2. Store keys in an array, parentIDs in another array.
	- Similar to what we did in disjointSets.
3. Store keys in an array
	- index can express the parent-child relationship.
	- only work for “complete” trees.
	- 3B: leaving one empty spot, making computation of child/parent nicer:
		- leftChild(k) = k\*2;
		- rightChild(k) = k\*2 + 1;
		- parent(k) = k/2

## Lecture 25: Advanced Trees: traversals and geometric trees

## Lecture 26: Graphs1
Some graph-processing problems:
- s-t Path: is there a path between nodes s and t;
- Shortest s-t path;
- Cycle: Does the graph contain any cycles;
- **Euler Tour**: a cycle using every path exactly once;
- **Hamilton Tour**: a cycle using every nodes exactly once;
- Connectivity: is the graph connected, is there a path between all node pairs;
- Biconnectivity: a node that disconnects the graph if removed;
- Planarity: draw a graph on a piece of paper with no crossing edges;
- **Isomorphism**: are two graphs isomorphic.

### Graph Representations

Define the Graph API first:
```java
public class Graph {
	/** Create an empty graph with V nodes. */
	public graph(int V);
	/** Add an edge v-w. */
	public void addEdge(int v, int w);
	/** Nodes adjacent to node v. */
	Iterable<Integer> adj(int v);
	/** Number of edge. */
	int E();
	/** Number of nodes/vertices. */
	int V();
	...
}

/** example of client that uses a Graph instant,
	return the degree of vertex v in graph G. */
public static int degree(Graph g, int v) {
	int degree = 0;
	for(int w: g.adg(v))
		degree++;
	return degree;
}
```

**Representation 1: Adjacency Matrix**
Suppose there are N nodes in a graph, we create a matrix edge[N][N]. edge[i][j] = 1 if there is a link from i to j, and 0 if there isn’t.

**Representation 2: Edge Sets**, collection of all edges.
HashSet\<Edge\>, where each edge is a pair of ints.

**Representation 3: Adjacency lists**
Maintain array of lists indexed by vertex number, in which every list contains its adjacent nodes.
> Most popular approach for representing graphs.

### Depth-First Traversal
Explore the entire subgraph for each child.

**Common design pattern in graph algorithms**: decouple type from processing algorithm, that is:
- Create a graph object;
- Pass the graph to a graph-processing method/constructor in a client class (which is designed for a specific task);
- Query the client class for information (result).

```java
// Take paths with Depth-First Search for example
public class Paths {
	private boolean[] marked;
	private int[] edgeTo;

	/** Find all paths from s in G. */
	public Paths(Graph G, int s);
	/** is there a path from s to v? */
	boolean hasPathTo(int v);
	/** path from s to v (if any). */
	Iterable<Integer> pathTo(int v);
}

Paths P = new Paths(G, 1); // find all paths from 1 in G.
P.hasPathTo(3);			   // a path from 1 to 3 exists?
p.pathTo(3);			   // return the path, like {1, 2, 0, 3}.
```

## Lecture 27: Graphs2 Traversals, DFS, Topological Sorting, BFS

### Depth First Search (DFS)
Compare to depth first path, DFS is a more general term for any graph search algorithm that traverses graph **as deep as possible before backtracking**.

### Graph Traversals

- DFS Postorder: the order of returns from DFS;
- DFS preorder: the order of DFS calls;
- Level traversal: more like **BFS**.

### Topological Sorting

### Breadth First Search (BFS)
Can also called Level-order Search that provides the shortest paths from 0 to every reachable vertex from 0.

BFS:
- Initialize a queue with a starting vertex and mark it. (call the vertex **fringe**)
- Repeat until queue is empty:
	- Remove vertex v from queue;
	- Add to the queue any unmarked vertices adjacent to v and mark them.

## Lecture 29: DFS vs. BFS, Shortest Paths

### Dijkstra’s Algorithm - Single Source Shortest Paths
BFS&DFS are enough for unweighted graphs to find the shortest paths between two nodes, but not for **weighted graphs**. Instead, we use **Dijkstra** to find the shortest paths from source node to every other node.

For Single Source Shortest Paths, solutions will always be **trees**:
- Can’t include cycles;
- Every node has one parent.
	- If one path is longer, it shouldn’t be part of the solution;
	- If they are the same, we can just pick one arbitrarily.
> Here, we can such a tree **Shortest Paths Tree (SPT)**.

#### Dijkstra’s Algorithm
Visit vertices in order of **best-known distance** from source, **relaxing** each edge from the visited vertex.
> Relaxing an edge means adding to SPT if better distance.

Though Dijkstra is a great algorithm in terms of finding shortest paths from single source to all other targets, it’s not good for finding the shortest path to a single target, which is useful for a navigation application.

> Also Note that if edge weights are all equal, Dijkstra’s algorithm is just BFS.

#### A\*
For instance, we want to find the shortest path from Denver to NY, a simple idea is, visiting vertices in order of *d(Denver, v) + h(v)*, where *h(v)* is an estimate of the distance from v to NY (target).
> Compared to Dijkstra’s which only consider d(source, v).

## Lecture 30: Minimum Spanning Trees (MST)
Given an undirected graph, a *spanning tree* T is a subgraph of G where T:
- is connected;
- is acyclic;
- includes all of the vertices.

A Minimum Spanning Tree is a spanning tree of minimum total weight.
> A SPT depends on the start vertex from which we go to everything, while a MST has no source since the total weight is the only thing we consider.

### Cut Property
A useful tool for finding the MST.

**Cut**: an assignment of a graph’s nodes to two non-empty sets.
**Crossing edge**: an edge which connects a node from one set to a node from the other set.
**Cut property**: Given any cut, minimum weight crossing edge is in the MST.

### Prim’s Algorithm
Start from some arbitrary start node,
- Repeatedly add shortest edge that has one node inside the MST under construction.
- Repeat until V-1 edges.

Prim’s and Dijkstra’s are exactly the same algorithm, except Dijkstra’s considers “distance from the source”, and Prim’s considers “distance from the tree (MST)”.

### Kruskal’s Algorithm
Initially mark all edges gray,
- Consider edges in increasing order of weight.
- Add edge to MST (mark black) unless doing so creates a cycle.
- Repeat until V-1 edges.


## Lecture 31: Dynamic Programming

### Shortest Paths in a DAG
 Any *Directed Acyclic Graph (DAG)* can be topological sorted.
 
### Dynamic Programming
The DAG SPT algorithm can be thought of as solving increasingly large subproblems:
- Distance from source to source is very easy, just zero;
- We then tackle distances to vertices that are a bit farther to the right;
- We repeat this until we get all the way to the end of the graph.

This approach of solving increasingly large subproblems is called **dynamic programming**.

### Longest Increasing Subsequence (LIS)
Elements in a LIS is only needed to be increasing, not to be neighboring.

**LIS using Dynamic Programming**
Suppose we use Q(i) to represent the Length of Longest Subsequence Ending at i (LLSEA). Q(j) will be equal to *Max(Q(i) + 1) for any i\<j if i is connected to j*.

The LIS using DP solve LIS problem as below:

- Initialization
	- Create Q(N). Set Q[0]=1, Q[K] = -infinity for K\>0.
	- Create a DAG with N vertices. Draw an edge from i to j (i\<j) if vertex i is less than vertex j.
- For K in range(1, N+1):
	- For L in range(0, K):
		- if there exists an edge from L to K:
			- if Q(L)+1 \> Q(K), set Q(K) = Q(L) + 1

Initialization runtimes of Init 1 and Init 2 are \Theta(N) and \Theta(N^2), respectively. Execution runtime is \Theta(N^2). So the total runtime is \Theta(N^2).

We can optimize this solution by not creating a DAG. So in execution period, we will check if item L is less than item K instead of if there exists an edge, though these two ways are the same. After this optimization, the runtime of the algorithm is \Theta(N^2) in the **worst case**.

## Lecture 32: Sorting1, Selection|Heap|Merge|Insertion|Shell’s Sort

An ***inversion*** is a pair of elements that are out of order.
> Goal of sorting: perform a sequence of operations that reduces inversions to 0.

### Selection Sort and Heapsort

**Selection Sort**
- Find the smallest item.
- Swap this item to the front and ‘fix’ it.
- Repeat for unfixed items until all items are fixed.

Use \Theta(N^2) time if we use an array-like data structure.
> Inefficient: look through entire remaining array every time to find the minimum.

**Naive Heapsort**
> Maintain a heap so that getting the minimum is fast.
- Insert all items into a max heap. Discard input array and create output array.
- Repeat N times:
	- Delete largest item from the max heap.
	- Put largest item at the end of the unused part of the output array.

**In-place Heapsort**
> The downside of naive heapsort is it uses extra space, which is \Theta(N).

Rather than building a new N+1 array as heap, we can treat input array as a heap. When the maximum is deleted from the heap, the last position of the heap is left, which is also the position the maximum should be in after sorted.

- “Bottom-up heapification” to convert the array into a heap.
	- Sink nodes in reverse level order: sink(k);
	- after sinking, guaranteed that tree rooted at position k is a heap.
- Repeat N times:
	- Delete largest item from max heap, swapping root with last item in the heap.

### Mergesort
Time complexity: \Theta(N logN); space complexity: \Theta(N).
> Faster than heap sort.
> Possible to do in-place merge sort, but algorithm is complicated and runtime performance suffers by a significant constant factor.

- Split items into 2 roughly even pieces.
- Mergesort each half (recursively).
- Merge the two sorted halves to form the final result.

### Insertion Sort

**General Strategy**
- Starting with an empty output sequence.
- Add each item from input, inserting into output at right position.

**In-place Insertion Sort**
- For items from 1 to N-1:
	- Designate item i as the traveling item.
	- Left swap item until traveller is in the right place.

> For arrays that are almost sorted, insertion sort does very little work. (Efficient in such a situation)

### Shell’s Sort: Optimized Insertion Sort
Fix multiple inversions at once (instead of 1 inversion per operation like naive insertion sort).

- Instead of comparing adjacent items, compare items that are one stride length h apart.
- Start with larger stride, and decrease towards 1.
- Example: h = 7, 3, 1. Time complexity is \Theta(N^1.5).

The idea here is by using large strides, fixes most of the inversions first.

## Lecture 33: Sorting 2, Quicksort

The core idea of quicksort: Partitioning. To partition an array a[] on element x=a[i] is to rearrange a[] so that:
- x moves to position j;
- all entries to the left of x are \<= x;
- all entires to the right of x are \>= x.

**Quicksort**
- Partition on leftmost item.
- Quicksort left half.
- Quicksort right half.

The best and average runtimes of Quicksort are both \Theta(NlogN), but the worst runtime of it is \Theta(N^2).

**Avoid the Quicksort worst case**
Bad ordering: array already in sorted order; bad elements: array with all duplicates.

Four philosophies:
1. Randomness: pick a random pivot or shuffle before sorting;
2. smarter pivot selection: calculate or approximate the median;
3. introspection: switch to a safer sort if recursion goes to deep;
4. preprocess the array: analyze array to see if Quicksort will be slow.

## Lecture 34: Sorting 3

**In-place Partitioning Scheme of Quicksort**
Two pointers pointing the first item and the last item respectively, and walk towards each other,
- left pointer loves small items;
- right pointer loves large items;
- swap anything they don’t like.

**Smarter Pivot Selection (linear time pivot pick)**
The best possible pivot is the *median* that splits the problem into two problems of size N/2.
> ***BFPRT*** algorithm can find the median in \Theta(N) time, but rarely used in practice.

> In Java, Arrays.sort(someArray) uses Mergesort if someArray consists of Objects, Quicksort if someArray consists of primitives.

### Shuffling

